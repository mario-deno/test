{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#The MNIST dataset comprises 60,000 training examples and 10,000 test examples \n",
    "#of the handwritten digits 0–9, formatted as 28x28-pixel monochrome images.\n",
    "\n",
    "#We store the training feature data (the raw pixel values for 60,000 images of hand-drawn digits) \n",
    "#and training labels (the corresponding value from 0–9 for each image) \n",
    "#as numpy arrays in train_data and train_labels, respectively. \n",
    "#Similarly, we store the evaluation feature data (10,000 images) and evaluation labels in eval_data and eval_labels, \n",
    "#respectively.\n",
    "\n",
    "# train_data is an array of matrix 28x28. 0 is black 255 is white\n",
    "((train_data, train_labels), (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_model_fn function, which conforms to the interface expected by TensorFlow's Estimator\n",
    "#mode:TRAIN training phase, EVAL: testing phase, PREDICT:inferencing,predicting phase\n",
    "#A tensor is a generalization of vectors and matrices to potentially higher dimensions.\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    #expect input tensors to have a shape of [batch_size, image_height, image_width, channels]\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1]) #-1 means dynamic batch_size holding the size of all other dimensions constant.  \n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    #We want to apply 32 5x5 filters to the input layer, with a ReLU activation functio\n",
    "    conv1 = tf.layers.conv2d(\n",
    "    inputs=input_layer,\n",
    "    filters=32,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu)\n",
    "    \n",
    "    #Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 32]\n",
    "    \n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    #Our output tensor (pool1) has a shape of [batch_size, 14, 14, 32]: the 2x2 filter reduces height and width by 50% each.\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    \n",
    "    #Our output tensor (conv2) has a shape of [batch_size, 14, 14, 64] . WHY 64??\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    #pool2 has shape [batch_size, 7, 7, 64]\n",
    "\n",
    "    \n",
    "  # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) #reshape input as linear vector: -1 preserves batch size\n",
    "    \n",
    "    #dense layer performs outputs = activation(inputs * kernel + bias) \n",
    "    #The units argument specifies the number of output neurons in the dense layer (1,024). 7*7*64*batch_size neurons mapped to 1024 neurons\n",
    "    #activation function is relu\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "    #Our output tensor dense has shape [batch_size, 1024].\n",
    "    \n",
    "    \n",
    "    #dropping neurons to prevent over-fitting\n",
    "    #The rate argument specifies the dropout rate; here, we use 0.4, which means 40% of the elements will be randomly dropped out during training.\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    #Our output tensor dropout has shape [batch_size, 1024].\n",
    "    \n",
    "\n",
    "  # Logits Layer\n",
    "  #The final layer in our neural network is the logits layer\n",
    "  #with 10 neurons (one for each target class 0–9), with linear activation (by default if not specified): y = c * input\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  #on logits layer our class is the max argument of the tensor [0,9]\n",
    "  #and his probability is equal to softwax funtion value [0,1]\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    #if predict mode return a model instance to be run by an Estimator.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes) as softmax cross entropy between label and logits\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        #train_op = minimize loss function with gradient descent\n",
    "        train_op = optimizer.minimize(\n",
    "          loss=loss,\n",
    "          global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "plt.gray() #use gray scale to show image\n",
    "x = train_data[123]\n",
    "plt.imshow(x) #show third image\n",
    "print(train_labels[123]) #print third image label  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da capire perché divide i valori dei pixel per 225 per portarli nel range [0,1]\n",
    "train_data = train_data/np.float32(255)\n",
    "train_labels = train_labels.astype(np.int32)  # to convert the tensor type into int32 due to limitation of type in estimator\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)  # to convert the tensor type into int32 due to limitation of type in estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_task_id': 0, '_service': None, '_global_id_in_cluster': 0, '_model_dir': '/tmp/mnist_convnet_model', '_master': '', '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f16041fbd30>, '_task_type': 'worker', '_save_checkpoints_steps': None, '_experimental_distribute': None, '_keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_device_fn': None, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_tf_random_seed': None, '_num_worker_replicas': 1, '_train_distribute': None, '_save_summary_steps': 100, '_protocol': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None}\n"
     ]
    }
   ],
   "source": [
    "#create an Estimator (a TensorFlow class for performing high-level model training, evaluation, and inference) for our model\n",
    "\n",
    "#Estimator object wraps a model which is specified by a model_fn. \n",
    "#model_dir is the directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into an estimator to continue training a previously saved model.\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-1421\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1421 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.418833, step = 1422\n",
      "INFO:tensorflow:Saving checkpoints for 1431 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 1.4246651.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f16041fb4e0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=400,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "# train one step and display the probabilties\n",
    "#mnist_classifier.train(\n",
    "#    input_fn=train_input_fn,\n",
    "#    steps=1,\n",
    "#    hooks=[logging_hook]) \n",
    "\n",
    "#TRAIN 100 epoch\n",
    "mnist_classifier.train(input_fn=train_input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-07T16:12:38Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-1431\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-07-16:12:47\n",
      "INFO:tensorflow:Saving dict for global step 1431: accuracy = 0.7633, global_step = 1431, loss = 1.3813535\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1431: /tmp/mnist_convnet_model/model.ckpt-1431\n",
      "{'accuracy': 0.7633, 'global_step': 1431, 'loss': 1.3813535}\n"
     ]
    }
   ],
   "source": [
    "#TEST MODEL\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1, #so that the model evaluates the metrics over one epoch of data\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PREDICT values\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "predict_results = mnist_classifier.predict(\n",
    "    input_fn = predict_input_fn\n",
    ")\n",
    "\n",
    "\n",
    "for el in predict_results:\n",
    "    print(el)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
