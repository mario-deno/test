{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "81d4cae0-9298-48f2-adba-dcd37b4edf0c"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "9b85e627-42b9-4bdf-8ede-245e31ddea73"
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(hist, stop=50):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "                            \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    axes[0].plot(range(stop), hist['acc'], label='Training', color='#FF533D')\n",
    "    axes[0].plot(range(stop), hist['val_acc'], label='Validation', color='#03507E')\n",
    "    axes[0].set_title('Accuracy')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].legend(loc='lower right')\n",
    "                             \n",
    "    axes[1].plot(range(stop), hist['loss'], label='Training', color='#FF533D')\n",
    "    axes[1].plot(range(stop), hist['val_loss'], label='Validation', color='#03507E')\n",
    "    axes[1].set_title('Loss')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].legend(loc='upper right')\n",
    "                             \n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "f23c116d-ec50-4782-9065-bffeded2673d"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False,  input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "a82da2cc-89ab-49c2-a674-4f761182cf6f"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "30e0688c-f5fd-4a51-a788-9165705594a9"
    }
   },
   "outputs": [],
   "source": [
    "x=model.output #?? quanti layer densi devo usare? e con quale numero di neuroni\n",
    "x=Flatten(input_shape=model.output_shape[1:])(x)\n",
    "x=Dense(256, activation='relu',  kernel_regularizer=l2(0.01))(x)\n",
    "x=Dropout(0.5)(x)\n",
    "preds=Dense(1, activation='sigmoid')(x) #dovrei usare softmax con due neuroni come output??\n",
    "\n",
    "final_model=Model(inputs=model.input,outputs=preds)\n",
    "#specify the inputs\n",
    "#specify the outputs\n",
    "#now a model has been created based on our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_path = 'whole_or_demaged_model.h5'\n",
    "\n",
    "if os.path.isfile(fine_tuned_model_path):\n",
    "    final_model.load_weights(fine_tuned_model_path) #load pretrained cnn on whole-or-damaged problem\n",
    "    print('weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "da5052e0-72c0-4311-be03-7af1e4f762fb"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1840 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of \\nimages with shape (batch_size (a.k.a. # of images), target_size, channels) \\nand y is a numpy array of corresponding labels.\\n\\nexample of image 2x2 \\n[BATCH\\n    [   IMAGE1\\n        [    ROW1\\n            [r,g,b pixel (1,1)] , [r,g,b pixel(1,2)]\\n        ],\\n        [    ROW2\\n            [r,g,b pixel (1,2)] , [r,g,b pixel(2,2)]\\n        ]\\n    ],\\n    [   IMAGE2\\n        .....\\n    ]\\n]\\n\\nbatches[0] ->  x the numpy array \\nbatches[0][0] -> first image\\nbatches[0][1] -> second image\\nbatches[0][0][0] -> first image first row\\nbatches[0][0][0][0] -> first image first row first pixel (rgb)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255) #divide all pixel value for 225\n",
    "\n",
    "\n",
    "#questo generator sembra creare immagini custom a partire dalla cartella specificata\n",
    "train_generator=train_datagen.flow_from_directory('/home/denote/data/car-damage-dataset/data1a/training',\n",
    "                                                 target_size=(256,256),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=20, #number of images for cycle\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=False)\n",
    "\n",
    "\n",
    "'''A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of \n",
    "images with shape (batch_size (a.k.a. # of images), target_size, channels) \n",
    "and y is a numpy array of corresponding labels.\n",
    "\n",
    "example of image 2x2 \n",
    "[BATCH\n",
    "    [   IMAGE1\n",
    "        [    ROW1\n",
    "            [r,g,b pixel (1,1)] , [r,g,b pixel(1,2)]\n",
    "        ],\n",
    "        [    ROW2\n",
    "            [r,g,b pixel (1,2)] , [r,g,b pixel(2,2)]\n",
    "        ]\n",
    "    ],\n",
    "    [   IMAGE2\n",
    "        .....\n",
    "    ]\n",
    "]\n",
    "\n",
    "batches[0] ->  x the numpy array \n",
    "batches[0][0] -> first image\n",
    "batches[0][1] -> second image\n",
    "batches[0][0][0] -> first image first row\n",
    "batches[0][0][0][0] -> first image first row first pixel (rgb)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "2045f0a5-5c51-45e8-9352-f56ae91fc0ea"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 460 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255) #divide all pixel value for 225\n",
    "\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory('/home/denote/data/car-damage-dataset/data1a/validation',\n",
    "                                                           target_size=(256,256),\n",
    "                                                           color_mode='rgb',\n",
    "                                                           batch_size=20, #number of images for cycle\n",
    "                                                           class_mode='binary',\n",
    "                                                           shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "ec155d36-0247-4b91-ae34-f3f2bde645df"
    }
   },
   "outputs": [],
   "source": [
    "nb_train_samples = train_generator.n//train_generator.batch_size #number of image / batch size \n",
    "nb_validation_samples = validation_generator.n // validation_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "1f669e6a-6978-4f0f-853d-c6d1c8e91a81"
    }
   },
   "outputs": [],
   "source": [
    "final_model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "186866f8-58f4-4e2b-8c86-06107552d59f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 372s 4s/step - loss: 4.6095 - acc: 0.8402 - val_loss: 4.9155 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.60435, saving model to whole_or_demaged_model.h5\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 371s 4s/step - loss: 4.7026 - acc: 0.7696 - val_loss: 4.8028 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.60435 to 0.65000, saving model to whole_or_demaged_model.h5\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 371s 4s/step - loss: 4.6663 - acc: 0.7777 - val_loss: 4.7859 - val_acc: 0.6739\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.65000 to 0.67391, saving model to whole_or_demaged_model.h5\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 371s 4s/step - loss: 4.6332 - acc: 0.8022 - val_loss: 4.7759 - val_acc: 0.6543\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.67391\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 371s 4s/step - loss: 4.6344 - acc: 0.7772 - val_loss: 4.7300 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67391 to 0.68913, saving model to whole_or_demaged_model.h5\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 371s 4s/step - loss: 4.6105 - acc: 0.7864 - val_loss: 4.7037 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.68913 to 0.70000, saving model to whole_or_demaged_model.h5\n",
      "Epoch 7/50\n",
      "22/92 [======>.......................] - ETA: 3:46 - loss: 4.8110 - acc: 0.6545"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_acc', #save weights setting monitoring the best accuracy of evaluation dataset\n",
    "                                 verbose=1, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='auto')\n",
    "\n",
    "fit_history = final_model.fit_generator(train_generator,\n",
    "                          steps_per_epoch=nb_train_samples,\n",
    "                          epochs=50,\n",
    "                          validation_data=validation_generator,\n",
    "                          validation_steps=nb_validation_samples,\n",
    "                          verbose=1,\n",
    "                          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6544e3a1-bee6-40b1-b052-2cd322a5d9f3"
    }
   },
   "outputs": [],
   "source": [
    "plot_metrics(fit_history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

